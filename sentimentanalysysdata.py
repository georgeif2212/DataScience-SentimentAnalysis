# -*- coding: utf-8 -*-
"""SentimentAnalysysData

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ANQTeqZvA9jYt669UxJOIYY-fXRU0J8G

# Análisis de sentimientos de tweets en aerolíneas de Estados Unidos

### Carga de librerías y *dataset*
"""

from google.colab import drive
import pandas as pd

# Montar Google Drive
drive.mount('/content/drive')

"""Carga del archivo .CSV"""

# Ruta del archivo en tu Google Drive
file_path = '/content/drive/MyDrive/UAM/UTB/DataScience/TweetsAirline.csv'

# Cargar el archivo en un DataFrame
df = pd.read_csv(file_path)
print("Shape original:", df.shape)
df.head()

"""## Instalación de librerías y recursos necesarios para manipular correctamente el **dataset**

### Instalación de librería NLTK (Natural Language Toolkit)
"""

!pip install nltk --quiet

"""
### Descarga de recursos esenciales desde NLTK:
- stopwords: lista de palabras comunes (como "the", "is", "in") que no aportan mucho significado.
- wordnet: base de datos léxica del inglés, usada para lematización.
- punkt: modelo de tokenización (separa el texto en palabras).
- omw-1.4: datos adicionales multilingües para la lematización con WordNet.
"""

!python -m nltk.downloader stopwords wordnet punkt omw-1.4

"""### Importación de módulos de NLTK"""

import re
import nltk
from nltk.corpus import stopwords          # Sirve para obtener las stopwords en inglés
from nltk.tokenize import word_tokenize    # Sirve para dividir el texto en palabras individuales
from nltk.stem import WordNetLemmatizer    # Sirve para convertir las palabras a su forma base o raíz
nltk.download('punkt_tab')

"""## Limpieza del dataset"""

# Se seleccionan solo las columnas con las que trabajaremos
df = df[['tweet_id', 'airline', 'retweet_count', 'text', 'tweet_created']]
print("Shape después de eliminar columnas:", df.shape)
df.head()

# Creamos un conjunto de "stop words" en inglés.
# Estas son palabras comunes que generalmente se eliminan porque no aportan mucho significado al análisis (como: 'the', 'and', 'but').
stop_words = set(stopwords.words('english'))

print(stop_words)
# Inicializamos el lematizador WordNet.
# La lematización transforma una palabra a su forma raíz (por ejemplo: "running" → "run") conservando el significado.
lemmatizer = WordNetLemmatizer()

# Función para limpiar y lematizar texto
def preprocess_text(text):
    # 1. Pasar a minúsculas para estandarizar
    text = text.lower()

    # 2. Eliminar URLs (http, https y www)
    text = re.sub(r'http\S+|www.\S+', '', text)

    # 3. Eliminar menciones (@usuario) y hashtags (#hashtag)
    text = re.sub(r'@\w+|#\w+', '', text)

    # 4. Eliminar signos de puntuación y caracteres especiales
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # 5. Eliminar espacios adicionales
    text = re.sub(r'\s+', ' ', text).strip()

    # 6. Tokenizar (convertir el texto en una lista de palabras individuales)
    tokens = word_tokenize(text)

    # 7. Eliminar stop words y aplicar lematización
    cleaned_tokens = [
        lemmatizer.lemmatize(word)
        for word in tokens
        if word not in stop_words
    ]

    # 8. Unir las palabras limpias en una sola cadena
    return ' '.join(cleaned_tokens)

"""### Aplicar función a la columna "text" del dataset"""

# Aplicar preprocesamiento al dataset
df['clean_text'] = df['text'].apply(preprocess_text)

# Ver los primeros resultados comparando texto original y limpio
df[['text', 'clean_text']].head()

"""## Instalación y uso del modelo VADER"""

!pip install vaderSentiment --quiet

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Inicializar el analizador
analyzer = SentimentIntensityAnalyzer()

"""Usamos la librería vaderSentiment, que ofrece un analizador preentrenado para texto informal.

A cada texto limpio (clean_text) se le aplica polarity_scores, que devuelve cuatro valores (neg, neu, pos, compound).

Nos quedamos con compound, una puntuación entre -1 y 1.

Si

compound >= 0.05 → positivo

compound <= -0.05 → negativo

entre ambos → neutral
"""

def get_sentiment_label(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# Aplicar análisis de sentimiento
df['compound_score'] = df['clean_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])
df['sentiment'] = df['compound_score'].apply(get_sentiment_label)

df[['clean_text','text','compound_score','sentiment',]].head(10)

"""# Estadísticas agregadas"""

import pandas as pd

# Porcentaje por categoría de sentimiento
sentiment_counts = df['sentiment'].value_counts(normalize=True) * 100
print("Porcentaje por sentimiento:")
print(sentiment_counts)

# Promedio de compound score por categoría
avg_scores = df.groupby('sentiment')['compound_score'].mean()
print("\nPuntuación compuesta promedio por sentimiento:")
print(avg_scores)

import matplotlib.pyplot as plt
import seaborn as sns

# Estilo bonito
sns.set(style="whitegrid")

# Gráfico de barras del porcentaje
plt.figure(figsize=(6, 4))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette="pastel")
plt.title("Porcentaje de Comentarios por Sentimiento")
plt.ylabel("Porcentaje")
plt.xlabel("Sentimiento")
plt.tight_layout()
plt.savefig("grafico_porcentaje_sentimiento.png")  # Si quieres guardar
plt.show()

!pip install wordcloud --quiet

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Crear nubes de palabras por cada sentimiento
for sentiment in ['positive', 'neutral', 'negative']:
    # Unimos todos los textos en uno solo
    text = ' '.join(df[df['sentiment'] == sentiment]['clean_text'])

    # Crear la nube de palabras
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Set2').generate(text)

    # Mostrar la nube
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Nube de Palabras - {sentiment.capitalize()}')
    plt.tight_layout(pad=0)

    # Guardar la imagen si lo deseas
    plt.savefig(f"wordcloud_{sentiment}.png")
    plt.show()

# Convertir a datetime
df['tweet_created'] = pd.to_datetime(df['tweet_created'])

# Extraer solo la fecha (sin hora)
df['tweet_date'] = df['tweet_created'].dt.date


# Agrupar por fecha y sentimiento
daily_sentiment = df.groupby(['tweet_date', 'sentiment']).size().unstack().fillna(0)

# Gráfico de línea para series temporales
plt.figure(figsize=(12, 6))
daily_sentiment.plot(kind='line', figsize=(12, 6), marker='o', colormap='Set1')

plt.title('Evolución Temporal de Sentimientos')
plt.xlabel('Fecha')
plt.ylabel('Cantidad de Tweets')
plt.grid(True)
plt.tight_layout()
plt.legend(title='Sentimiento')
plt.savefig("serie_temporal_sentimientos.png")
plt.show()

"""## Exportar en JSON para posteriormente cargar en WEB

"""

import json
from collections import defaultdict

# Cargar tweets
tweets_data = df[['tweet_id', 'clean_text', 'sentiment', 'tweet_created']].copy()
tweets_data['tweet_created'] = tweets_data['tweet_created'].astype(str)  # Convertir datetime a string
tweets_list = tweets_data.to_dict(orient='records')

# Cargar summary

# Conteos
sentiment_counts = df['sentiment'].value_counts().to_dict()

# Porcentajes
total = len(df)
sentiment_percentages = {k: round(v / total * 100, 2) for k, v in sentiment_counts.items()}

summary_data = {
    "count": sentiment_counts,
    "percentage": sentiment_percentages
}
print(summary_data)

wordclouds_data = {
    "positive": "wordcloud_positive.png",
    "neutral": "wordcloud_neutral.png",
    "negative": "wordcloud_negative.png"
}

bar_chart_data = [
    {"sentiment": k, "count": v, "percentage": sentiment_percentages[k]}
    for k, v in sentiment_counts.items()
]

timeline_data = []

# daily_sentiment viene del paso anterior
for date, row in daily_sentiment.iterrows():
    timeline_data.append({
        "date": str(date),
        "positive": int(row.get("positive", 0)),
        "neutral": int(row.get("neutral", 0)),
        "negative": int(row.get("negative", 0))
    })

final_data = {
    "tweets": tweets_list,
    "summary": summary_data,
    "bar_chart": bar_chart_data,
    "wordclouds": wordclouds_data,
    "timeline": timeline_data
}

# Guardar a JSON
with open("sentiment_results.json", "w", encoding='utf-8') as f:
    json.dump(final_data, f, indent=2, ensure_ascii=False)